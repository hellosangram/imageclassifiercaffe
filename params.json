{
  "name": "Image Classifier Caffe",
  "tagline": "Tutorial on how to setup a machine for a Deep Learning application, in this case Image Classification.  ",
  "body": "### Part 1: Setting up the machine for Deep Learning.\r\nThis guide is intended for people with similar laptop configurations as mine. I have an ****Asus301U, i5 4th Gen CPU, 8gB RAM, 2gb Nvidia Graphics Card 940M, UEFI BIOS system and Ubuntu 16.04 installed as a dual-boot option.****  \r\nMinor hiccups might come during the installation but that is the fun part to figure them out and I will leave them upto the individuals as most of them have already been resolved and just needs some digging.\r\n\r\n### Installation of Cuda\r\nFirst thing to remember is that Cuda and Ubuntu versions should be inter compatible, I initially chose Cuda7.5 with Ubuntu 16.04, and it did not along well thus I had to reinstall Cuda8.0 to have it run on my machine. I will abstain from repeating the information again and thus post threads of people who have already listed the procedure.  \r\nFollow this thread for Cuda installation. http://askubuntu.com/questions/799184/how-can-i-install-cuda-on-ubuntu-16-04\r\nProblems you might encounter and how you might resolve them.\r\n* Give permission for UEFI insecure boot if required, this will allow your Nvidia Graphics card drivers to work.\r\n* Stop lightdm server by pressing ctr+alt+f2 and then stopping it. Go ahead finish the cuda installation and restart it.\r\n* Cuda sometimes install old drivers for graphics card, please update them accordingly. This is useful to resolve the issue of infinite loop login screen appearance.\r\n* Check Cuda version and your Graphics cards installation with standard commands.\r\n\r\n### Installation of Caffe\r\nFollow the link. http://caffe.berkeleyvision.org/install_apt.html along with http://hanzratech.in/2015/07/27/installing-caffe-on-ubuntu.html\r\nRemember to set your CPU and GPU flags according to your system specifications.\r\nDon't forget to update your .bashrc file with the paths for future ease of use.\r\nUpdate all the prerequisites so as to follow a smooth functioning.\r\n\r\n### Some basics about training your models.\r\nAfter you install caffe, you might have some models downloaded by default in your caffe file system, if need be then download their pre-trained files too.  \r\nNow for every training you have got three options to either retrain the whole network from scratch, fine tune an existing model, freeze certain layers and just modify the layers you require. Most of the time it depends on the use case on which one to adopt. \r\n\r\n###Part 2: Image Classifier for ISBI Challenge on Analysis of Images to Detect Abnormalities in Endoscopy (AIDA-E)\r\nLink to the challenge: https://isbi-aida.grand-challenge.org/\r\nMy goal to complete the challenge in minimum time was the biggest critereon, this methodology should never be allowed in machine learning as each problem requires gestation before attack. I also preferred using AlexNet CNN and not any other as the images provided were of infinitesimal magnitude in comparison to ImageNet dataset which means many of the better performing models were large and prone to over-fitting.  \r\nAnother reason for choosing this network was due to my computer specifications which unfortunately could not support any bigger model without reducing batch size which if reduced too much might affect accuracy along with the standard affect of time.  \r\nTo overcome the small dataset problem I used Keras libary for data augmentation which is fabolous to use. But again too much of augmentation from the same images can not help you save from over fitting.  \r\n  \r\n\r\n### Model Selection\r\n* _GoogleLeNet Caffe Implementation_ Present in Caffe Zoo, this model is very powerful when used for image classification. The drawback when I used was that it was too large too fit in my GPU memory and I saw preliminary results which showed superior prediction results on train image dataset and not on test image dataset. I used fine tuning of the model trained on ImageNet dataset. \r\n* _Siamses Network_ It is generally used for image similarity, but the unique thing is after training. It can be used on an image to detect its similarity to other just by giving one test image. I did not use this network due to the primary reason of implementation, for me I would have required to upload multiple images and create a median vector which could have then compared to test images. Theoretically sounds good but such implementation was nowhere to be found.\r\n* _Alexnet Caffe Implementation_ I used this network for its lack of magnitude of complexity and thus saving me from over-fitting in my case. The batch size I used was 64 which was decent considering the configuration of my laptop. I used freezing of certain layers and then finetuning of the last two layers. One change was to reduce the output paramter in fc7 from 4096 to 2048 and the other increasing the dropout to .75. These were done to reduce the oft used word by me \"over fitting\". I of course changed the output from 1000 in case of imagenet to 4 for our cases and the locations.   \r\n\r\n### Data Manipulation\r\n_Data Augmentation_ I used Keras library for data augmentation, the inbuilt function ImageDataGenerator created for a kaggle challenge and later adopted in the library is a beautiful and time saving tool.\r\n_Data Resizing_ I resized the data from standard 227*227 used for ImageNet to 1024*1024 which would surely have helped as medical images are very sensitive to data loss and I wanted to avoid that. But alas my laptop could not support loading such a monstrous dataset and I had to maroon the idea alone.\r\n_Data Division_ This idea came in but too late to implement that I could have created 20 images from one image of 1024*1024 by using standard 227*227 image thus making the data more mixed without augmentation.\r\n_Feature Marking in Data_ I was reading Kaggle competition for detecting individual whales by spotting their spouts and identifying it with them. They too had a small dataset of 4000(yes way bigger than ours but still). The approach they took was to make the network focus on those features, I wanted to implement this but it was a moonshot considering data annotation and introduction of a data layer for the network to have some manual features added.\r\n\r\nPart3: Run the implementation.\r\n_Run: createData.py_ A script to change the nomenclature to one I find suitable for easy use.\r\n_Run: dataAugmentation.py_ For creating augmented data.\r\n**Either** _Run: createLmdb.py_ Creates lmdb files for caffe to pick and process. Use this if data is not big.\r\n**Or** * _Run: createLmdbTxt.py_ Creates txt file containing the relative image paths for the ensuing .sh file\r\n       * _Run: create_lmdb.sh_ Used this .sh file as suggested in caffe issues if the data set becomes too large to hold in the memory, this files processes thousand at one time thus a life saver when I tested 1024*1024 images in AlexNet.\r\n\r\nCreate binary proto file for mean values using /home/sangram/caffe/build/tools/caffe train --solver=/home/sangram/deepLearning/deeplearning-medical-images/caffe_models/caffe_model_2/solver_2.prototxt 2>&1 | tee /home/sangram/deepLearning/deeplearning-medical-images/caffe_models/caffe_model_2/model_2_train.log command on terminal\r\n\r\nTrain Model: /home/sangram/caffe/build/tools/caffe train --solver=/home/sangram/deepLearning/deeplearning-medical-images/caffe_models/caffe_model_2/solver_2.prototxt --weights /home/sangram/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel 2>&1 | tee /home/sangram/deepLearning/deeplearning-medical-images/caffe_models/caffe_model_2/model_2_train.log\r\n\r\n_Run: makePredictions.py_ This will give you the output predictions which you can use for checking accuracy and other functions. I used excel as accuracy is just a unitary method to implement. \r\n\r\n  \r\n### Support or Contact\r\n sangram.gupta@gmail.com ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}